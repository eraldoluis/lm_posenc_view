# Interesting Patterns in BERT and GPT-2 Positional Encodings

Inspired by [von Werra's Twitter thread](https://twitter.com/lvwerra/status/1485301457813487619?s=20&t=Sni9NjYo5P1O1grGhFwXJw), 
  I present some visualizations that highlight 
    interesting patterns on GPT-2 and BERT positional encodings.
All visualizations can be found in the notebook [lm_posenc_view.ipynb](lm_posenc_view.ipynb).
I also published a [blog post](https://eraldoluis.github.io/2022/02/22/positional-encoding-visualization.html) 
  in which I discuss some relevant aspects of this kind of visualization, 
    and how some basic parameters have a high impact on the final result.
